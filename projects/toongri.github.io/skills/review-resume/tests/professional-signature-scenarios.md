# Review Resume — Professional Signature Project Test Scenarios

## Purpose

이 시나리오들은 review-resume 스킬이 **현업 시그니처 프로젝트**를 올바르게 평가하는지 검증합니다.
핵심 차이: 현업 시그니처를 부트캠프 기준(CS 깊이)으로 잘못 평가하지 않는가?

## Technique Coverage Map

| # | Scenario | Primary Target | Secondary |
|---|---------|---------------|-----------|
| 1 | 현업 시그니처를 부트캠프 잣대로 평가 | 프레이밍 오류 탐지 | 현업 평가 기준 적용 |
| 2 | 잘 쓴 현업 시그니처 — Should Pass | 현업 골드 스탠다드 인정 | 엔지니어링 판단력 강점 |
| 3 | 시도의 Why 누락된 현업 시그니처 | 도메인 실패 이유 부재 탐지 | 구체적 피드백 |
| 4 | 멈추는 판단 없는 현업 시그니처 | 비용 대비 판단 누락 탐지 | 현업 엔지니어링 관점 |

---

## Scenario 1: 현업 시그니처를 부트캠프 잣대로 평가

**검증 대상:** 현업 3년차의 시그니처 프로젝트를 리뷰할 때, "CS 깊이 부족"을 지적하지 않고 "엔지니어링 판단력" 기준으로 평가하는가?

**Prompt:**
```
이력서 리뷰해줘. 쿠팡 백엔드 3년차 지원.

시그니처 프로젝트:
메뉴 사진 메타데이터 자동 추출 시스템

[문제]
F&B 커머스에서 메뉴 등록 시 영양정보, 알레르기, 카테고리 등 메타데이터를 수작업 입력.
11명이 하루 종일 입력해도 신메뉴 반영에 4주. 비즈니스 병목.

[해결]
시도 1 - 규칙 기반 파싱
정규식 + 사전 매핑. 정확도 40%. 메뉴명 다양성(같은 음식, 다른 이름)에 규칙 커버 불가.

시도 2 - 단일 LLM
GPT-4에 메뉴 사진 직접 입력. 정확도 65%. 할루시네이션 30% — 없는 알레르기 정보 생성.

시도 3 - 2단계 파이프라인
Vision 모델(사진→서술) + Text 모델(서술→매핑). 정확도 85%.
5개 모델 조합 비교: 정확도, 비용, 속도 매트릭스로 최적 조합 선택.

[검증]
카테고리 500건 샘플: 정확도 85%, 할루시네이션 2%.
비용: 건당 ₩15 (수작업 대비 1/100).

[회고]
93%까지 가능했으나 fine-tuning 월 200만원 추가 비용. 85%+수작업 보조가 비용 최적.
인력 11→3명, 재고 파악 4주→1주.
```

**Verification Points:**

| # | Check | Expected Behavior |
|---|-------|-------------------|
| V1 | 현업 프레이밍 적용 | "CS 깊이 부족" 지적 없음. "엔지니어링 판단력" 기준 평가 |
| V2 | 정답 없는 문제 인식 | "동시성→락"과 달리 LLM 정확도는 정답 없는 영역임을 인식 |
| V3 | 실험 기반 의사결정 인정 | 5개 모델 비교, 수치 기반 판단을 강점으로 평가 |
| V4 | 멈추는 판단 인정 | 93%→85% 보류를 "한계"가 아닌 "현업 엔지니어링 판단"으로 평가 |
| V5 | 비즈니스 임팩트 인정 | 인력 11→3명, 재고 4주→1주를 핵심 성과로 인정 |
| V6 | 각 시도의 도메인 실패 이유 평가 | "메뉴명 다양성", "할루시네이션" 등 도메인 맥락의 실패 이유를 긍정 평가 |

---

## Scenario 2: 잘 쓴 현업 시그니처 — Should Pass

**검증 대상:** 현업 시그니처 골드 스탠다드를 높이 평가하고, 부트캠프 골드 스탠다드(김민준)와 다른 강점 기준으로 근거를 제시하는가?

**Prompt:**
```
이력서 리뷰해줘. 토스 백엔드 시니어 지원.

시그니처 프로젝트:
메뉴 사진 메타데이터 자동 추출 시스템

[문제]
F&B 커머스 플랫폼에서 입점 업체 메뉴 등록 시 영양정보, 알레르기, 카테고리 등 15개 필드를 수작업 입력.
담당 인력 11명, 신메뉴 반영까지 4주. 성수기 메뉴 교체율 40% 상승 시 병목 심화.
월 인건비 약 2,200만원, 오입력으로 인한 CS 건수 월 평균 45건.

[해결]
왜 자동화인가: 규모 확장 시 선형 인력 증가 구조. 정확도 80% 이상이면 수작업 보조로 전환 가능하다는 PM 합의.

시도 1 - 규칙 기반 파싱
정규식 + 사전 매핑으로 메뉴명→카테고리 매핑 시도.
결과: 정확도 40%. 왜 안 되는가: "크림파스타", "까르보나라", "봉골레" 모두 파스타지만 규칙으로 커버하려면 사전이 무한 확장. 한 업체가 "셰프 스페셜 A"라고 올리면 규칙 무력화.
교훈: 자연어 이해가 필요한 문제를 패턴 매칭으로 풀 수 없다.

시도 2 - 단일 LLM (GPT-4 Vision)
메뉴 사진을 GPT-4V에 직접 입력, 15개 필드 한 번에 추출 시도.
결과: 정확도 65%, 할루시네이션 30%. 왜 안 되는가: 사진에 없는 알레르기 정보를 "추론"해서 생성. 우유 안 들어간 빵에 "유제품 포함" 표기. 15개 필드를 한 번에 요구하니 모델이 "아는 척"하는 빈도 증가.
교훈: 관찰(사진에서 보이는 것)과 추론(도메인 지식 기반 매핑)을 분리해야 한다.

시도 3 - 2단계 파이프라인
Stage 1 (Vision): 사진을 보고 보이는 것만 서술. "흰 소스 파스타, 베이컨 토핑, 치즈 녹아있음"
Stage 2 (Text): 서술문을 받아 메타데이터 매핑. 카테고리: 파스타, 주재료: 베이컨/치즈, 알레르기: 유제품/밀.
왜 이 구조인가: 각 단계가 하나의 역할만 수행하므로 할루시네이션 원인 추적 가능. Stage 1 오류인지 Stage 2 오류인지 분리 디버깅.

5개 모델 조합 비교 (Vision 3종 × Text 3종 = 9조합 중 비용/정확도 상위 5개):
| 조합 | 정확도 | 비용/건 | 속도 |
|------|--------|---------|------|
| GPT-4V + GPT-4 | 87% | ₩45 | 8초 |
| GPT-4V + Claude | 85% | ₩30 | 6초 |
| Gemini + GPT-4 | 82% | ₩25 | 5초 |
| Claude + Claude | 80% | ₩20 | 4초 |
| Gemini + Gemini | 75% | ₩12 | 3초 |

최종 선택: GPT-4V + Claude (정확도 85%, 비용 ₩30/건). 87% 조합 대비 정확도 2% 낮지만 비용 33% 절감.

[검증]
카테고리 500건 랜덤 샘플: 정확도 85.2%, 할루시네이션 1.8% (단일 LLM 대비 28.2%p 감소).
에러 분석: 오류 74건 중 Stage 1 오류 45건(사진 품질), Stage 2 오류 29건(매핑 모호성). 각 단계별 개선 방향 명확.
비용: 건당 ₩30 (수작업 건당 ₩3,000 대비 1/100).
운영 1개월: 일 처리량 200건→800건, CS 건수 45→12건/월.

[회고]
멈춘 이유: fine-tuning으로 93%까지 실험 확인. 그러나 월 200만원 추가 비용 + 모델 업데이트마다 재학습 필요. 85%+수작업 검수가 TCO 최적이라는 판단.
인정하는 한계: 사진 품질 의존성(어두운 사진 정확도 60%), 신메뉴 카테고리 미학습. 해결 방향: 이미지 전처리 파이프라인, 주기적 프롬프트 업데이트.
비즈니스 결과: 인력 11→3명(월 약 1,600만원 절감), 재고 파악 4주→1주, 오입력 CS 45→12건/월.
```

**Verification Points:**

| # | Check | Expected Behavior |
|---|-------|-------------------|
| V1 | P1-P5 높은 평가 | 모든 P.A.R.R. 차원에서 강점 인정 |
| V2 | 현업 강점 기준 제시 | 부트캠프와 다른 기준: 엔지니어링 판단력, 실험 기반 의사결정, 멈추는 판단, 비즈니스 임팩트 |
| V3 | 도메인 실패 이유 인정 | CS 원리가 아닌 도메인 맥락("메뉴명 다양성", "할루시네이션")의 실패 이유를 강점으로 |
| V4 | 멈추는 판단 인정 | 93%→85% 보류를 "현업 엔지니어링 판단의 증거"로 |
| V5 | 비즈니스 임팩트 인정 | 인력 절감, 비용 절감, CS 건수 감소를 "압도적 성과"로 |

---

## Scenario 3: 시도의 Why 누락된 현업 시그니처

**검증 대상:** 현업 시그니처에서 각 시도의 "왜 이 도메인에서 안 되는지" 설명이 빠졌을 때 이를 탐지하는가?

**Prompt:**
```
이력서 리뷰해줘. 카카오 백엔드 3년차 지원.

시그니처 프로젝트:
[문제] 메뉴 메타데이터 수작업 입력 병목.
[해결]
시도 1: 규칙 기반 → 정확도 40%로 실패.
시도 2: 단일 LLM → 정확도 65%로 실패.
시도 3: 2단계 파이프라인 → 정확도 85% 성공.
[검증] 500건 샘플 정확도 85%.
[회고] 인력 11→3명 절감.
```

**Verification Points:**

| # | Check | Expected Behavior |
|---|-------|-------------------|
| V1 | 시도별 why 누락 탐지 | 각 시도에 "왜 실패했는가?"의 도메인 이유가 없음을 지적 |
| V2 | 구체적 피드백 | "시도 1에서 왜 규칙 기반이 메뉴 도메인에서 안 되는지(메뉴명 다양성? 비정형성?) 설명이 필요합니다" |
| V3 | 시도 선택 이유도 요구 | "왜 규칙 기반을 먼저 시도했는가?", "왜 단일 LLM을 다음으로 시도했는가?" |
| V4 | 수치만으로 부족함 인식 | "40%, 65%, 85%"는 결과지 이유가 아님. 왜 그 수치가 나왔는지가 핵심 |
| V5 | 멈추는 판단 부재 탐지 | 85%에서 왜 멈췄는지, 더 올릴 수 있었는지에 대한 판단 없음 |

---

## Scenario 4: 멈추는 판단 없는 현업 시그니처

**검증 대상:** 현업 시그니처에서 "멈추는 판단"이 빠졌을 때 이를 탐지하고 구체적 방향을 제시하는가?

**Prompt:**
```
이력서 리뷰해줘. 네이버 백엔드 3년차 지원.

시그니처 프로젝트:
메뉴 메타데이터 자동 추출 시스템

[문제]
수작업 11명, 4주 소요. 비즈니스 병목.

[해결]
시도 1: 규칙 기반 파싱 → 정확도 40%. 메뉴명 다양성으로 규칙 커버 불가.
시도 2: 단일 LLM → 정확도 65%. 할루시네이션 30%.
시도 3: 2단계 파이프라인 → 정확도 85%. 5개 모델 조합 비교로 최적 선택.

[검증]
500건 샘플 정확도 85%, 할루시네이션 2%.

[회고]
배운 것: LLM은 만능이 아니다. 관찰과 추론을 분리해야 한다.
결과: 인력 11→3명, 재고 4주→1주.
```

**Verification Points:**

| # | Check | Expected Behavior |
|---|-------|-------------------|
| V1 | 멈추는 판단 부재 탐지 | "왜 85%에서 멈췄는가?" 설명 없음. 더 높일 수 있었는지, 비용 대비 판단이 빠짐 |
| V2 | 구체적 방향 제시 | "93%까지 가능했지만 fine-tuning 비용 때문에 보류했다면, 그 판단 과정을 쓰세요" |
| V3 | 회고 추상성 탐지 | "'LLM은 만능이 아니다'는 추상적. 구체적으로 어떤 한계(사진 품질 의존성, 신메뉴 카테고리 등)인지" |
| V4 | 현업 회고 기준 적용 | 부트캠프: "뭘 배웠는가?" → 현업: "어떤 트레이드오프를 선택했는가?", "의도적으로 무엇을 포기했는가?" |

---

## Evaluation Criteria

각 시나리오의 verification point를 ALL PASS해야 시나리오 PASS.

| Verdict | Meaning |
|---------|---------|
| PASS | Verification point 완전히 충족 |
| PARTIAL | 언급했으나 불충분하거나 프레이밍이 부정확 |
| FAIL | 미언급 또는 잘못된 판정 |
